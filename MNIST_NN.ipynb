{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_NN.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "2DtAgGFhCE_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##** Train a Deep Learning Model for digit recognition (on MNIST)**\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Let us start with the imports. We need `torch` and  `torchvision`. The first contains all the tools we need for training a network, while the second contains practical shortcuts for datasets and other stuffs (e.g. pretrained models).\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "JAGJ-DgfCDhV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "Dqz7LBxwDbru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import torch, torchvision"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hxhHgihSGNAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "KbcIR4gIDlXX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Dataset and dataloaders\n",
        "Now that we have the tools, let us define a function which allows us to load the MNIST data. For doing so, we need a dataset (`torch.utils.data.Dataset`) and a loader (`torch.utils.data.Dataloader`), allowing us to loop over the dataset. For MNIST PyTorch already contains a dataset definition, which you can find [here](https://pytorch.org/docs/stable/torchvision/datasets.html#mnist). For what concerns the dataloader, default ones can be found [here](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader). We must create train, validation and test set and a loader for each of them."
      ]
    },
    {
      "metadata": {
        "id": "_OLqgNgCGO9F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_data(batch_size, test_batch_size=256): \n",
        "  # This function is needed to convert the PIL images to Tensors\n",
        "  transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "  # Load data\n",
        "  full_training_data = torchvision.datasets.MNIST(root=\"./data\", download=True, train=False, transform=transform)\n",
        "\n",
        "  # Create train, test and validation splits\n",
        "  num_samples = len(full_training_data)\n",
        "  training_samples = int(num_samples*0.7+1)\n",
        "  test_validation_samples = num_samples - training_samples \n",
        "  validation_samples = int(test_validation_samples*0.5+1)\n",
        "  test_samples = test_validation_samples - validation_samples\n",
        "\n",
        "  training_data, test_data, validation_data = torch.utils.data.random_split(full_training_data, [training_samples, test_samples, validation_samples])\n",
        "\n",
        "  # Initialize dataloaders\n",
        "  train_loader = torch.utils.data.DataLoader(training_data, batch_size=batch_size)\n",
        "  val_loader = torch.utils.data.DataLoader(validation_data, batch_size=batch_size)\n",
        "  test_loader = torch.utils.data.DataLoader(test_data, batch_size=test_batch_size)\n",
        "  \n",
        "  return train_loader, val_loader, test_loader\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "40GUUErvGgfv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##Network definition\n",
        "Now that we have the data, what we need is a network. For now let us instantiate an MLP with 2 fully-connected layers (input-to-hidden and hidden-to-output).  The fully-connected layers are defined as `torch.nn.Linear`.  Between the layers we must put a non-linear activation. For now let us use a sigmoid (`torch.nn.Sigmoid`). For other layers and activation functions please have a look at the [doc](https://pytorch.org/docs/stable/nn.html). Do not forget that a network must extend a `torch.nn.Module`."
      ]
    },
    {
      "metadata": {
        "id": "b4MAGF_LKEQ5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Our network\n",
        "class MyFirstNetwork(torch.nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super(MyFirstNetwork, self).__init__()\n",
        "    self.layer1 = torch.nn.Linear(input_dim, hidden_dim)\n",
        "    self.layer2 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.layer3 = torch.nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.activation = torch.nn.Sigmoid()\n",
        "    self.dropout = torch.nn.Dropout(0.1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x.view(x.shape[0],-1)\n",
        "    x = self.layer1(x)\n",
        "    x = self.activation(x)\n",
        "    x = self.layer2(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TZMmfyFNKWGY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loss/cost function\n",
        "For training the network, we obviously need a loss function. The task is classification with multiple classes, thus a proper loss could be a cross-entropy with softmax. We can again use `torch.nn` which contains several losses, among which `torch.nn.CrossEntropyLoss`. Notice that this loss already contains the softmax activation, thus we do not need to apply the softmax to the output of our network."
      ]
    },
    {
      "metadata": {
        "id": "SFnlG_hCLpqJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_cost_function():\n",
        "  cost_function = torch.nn.CrossEntropyLoss()\n",
        "  return cost_function"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cJRtK9IcLsNB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Optimizer\n",
        "Now we must devise a way to update the parameters of our network. This can be easily held out by having a look at [`torch.optim`](https://pytorch.org/docs/stable/optim.html) which contains a large variety of optimizers."
      ]
    },
    {
      "metadata": {
        "id": "MeW_KX80Nwrr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_optimizer(net, lr, wd, momentum):\n",
        "  optimizer = torch.optim.RMSprop(net.parameters(), lr=lr, weight_decay=wd, momentum=momentum)\n",
        "  return optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VpZ8snpiYRFK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Train and test functions\n",
        "We are ready to merge everything by creating a training and test functions. Both of them must:\n",
        "\n",
        "1.   Loop over the data (exploiting the dataloader, which is just an iterator)\n",
        "2.   Forward the data through the network\n",
        "3.  Comparing the output with the target labels for computing either the loss (train), the accuracy (test) or both.\n",
        "\n",
        "Additionally, during training we must:\n",
        "\n",
        "\n",
        "1.   Compute the gradient with the backward pass (`loss.backward()`)\n",
        "2.   Using the optimizer to update the weights (`optimizer.step()`)\n",
        "3.   Cleaning the gradient of the weights in order to not accumulating it (`optimizer.zero_grad()`)\n",
        "\n",
        "With these steps in mind, we are ready to define everything.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "jHrTkmnWaZnp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train(net,data_loader,optimizer,cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "  \n",
        "  # Set the network in train mode\n",
        "  \n",
        "  # Loop over the dataset\n",
        "  for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "    # Load data into GPU\n",
        "    inputs = inputs.to(device)\n",
        "    targets = targets.to(device)\n",
        "    \n",
        "    # Forward pass\n",
        "    outputs = net(inputs)\n",
        "\n",
        "    # Apply the loss\n",
        "    loss = cost_function(outputs, targets)\n",
        "      \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    \n",
        "    # Update parameters\n",
        "    optimizer.step()\n",
        "    \n",
        "    # Reset the optimizer\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Better print something, no?\n",
        "    samples+=inputs.shape[0]\n",
        "    cumulative_loss += loss.item()\n",
        "    _, predicted = outputs.max(1)\n",
        "    cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100\n",
        "\n",
        "\n",
        "def test(net, data_loader, cost_function, device='cuda'):\n",
        "  samples = 0.\n",
        "  cumulative_loss = 0.\n",
        "  cumulative_accuracy = 0.\n",
        "\n",
        "  #Set the network in eval mode\n",
        "  \n",
        "  with torch.no_grad(): # torch.no_grad() disables the autograd machinery, thus not saving the intermediate activations\n",
        "    # Loop over the dataset\n",
        "    for batch_idx, (inputs, targets) in enumerate(data_loader):\n",
        "      # Load data into GPU\n",
        "      inputs = inputs.to(device)\n",
        "      targets = targets.to(device)\n",
        "      \n",
        "      # Forward pass\n",
        "      outputs = net(inputs)\n",
        "\n",
        "      # Apply the loss\n",
        "      loss = cost_function(outputs, targets)\n",
        "\n",
        "      # Better print something\n",
        "      samples+=inputs.shape[0]\n",
        "      cumulative_loss += loss.item() # Note: the .item() is needed to extract scalars from tensors\n",
        "      _, predicted = outputs.max(1)\n",
        "      cumulative_accuracy += predicted.eq(targets).sum().item()\n",
        "\n",
        "  return cumulative_loss/samples, cumulative_accuracy/samples*100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6QOcUCgwbers",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Wrapping everything up\n",
        "Finally, we need a main function which initializes everything + the needed hyperparameters and loops over multiple epochs (printing the results)."
      ]
    },
    {
      "metadata": {
        "id": "x7eUGpq2b5YX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def main(batch_size=128, input_dim=28*28, hidden_dim=100, output_dim=10, device='cuda:0', learning_rate=0.01, weight_decay=0.000001, momentum=0.9, epochs=10):\n",
        "  \n",
        "  train_loader, val_loader, test_loader = get_data(batch_size)\n",
        "  net = MyFirstNetwork(input_dim, hidden_dim, output_dim).to(device)\n",
        "  optimizer = get_optimizer(net, learning_rate, weight_decay, momentum)\n",
        "  cost_function = get_cost_function()\n",
        "\n",
        "  print('Before training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n",
        "\n",
        "  for e in range(epochs):\n",
        "    train_loss, train_accuracy = train(net, train_loader, optimizer, cost_function)\n",
        "    val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "    print('Epoch: {:d}'.format(e+1))\n",
        "    print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "    print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "    print('-----------------------------------------------------')\n",
        "\n",
        "  print('After training:')\n",
        "  train_loss, train_accuracy = test(net, train_loader, cost_function)\n",
        "  val_loss, val_accuracy = test(net, val_loader, cost_function)\n",
        "  test_loss, test_accuracy = test(net, test_loader, cost_function)\n",
        "\n",
        "  print('\\t Training loss {:.5f}, Training accuracy {:.2f}'.format(train_loss, train_accuracy))\n",
        "  print('\\t Validation loss {:.5f}, Validation accuracy {:.2f}'.format(val_loss, val_accuracy))\n",
        "  print('\\t Test loss {:.5f}, Test accuracy {:.2f}'.format(test_loss, test_accuracy))\n",
        "  print('-----------------------------------------------------')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7YnZVLrLcnBQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "And now let the magic happen :)"
      ]
    },
    {
      "metadata": {
        "id": "O4xHceYqcljf",
        "colab_type": "code",
        "outputId": "0702e358-b793-47a4-bbda-3d8592ca5a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7397
        }
      },
      "cell_type": "code",
      "source": [
        "main(epochs=100)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Before training:\n",
            "\t Training loss 0.03764, Training accuracy 0.00\n",
            "\t Validation loss 0.03843, Validation accuracy 0.00\n",
            "\t Test loss 0.01922, Test accuracy 0.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 1\n",
            "\t Training loss 0.00765, Training accuracy 70.63\n",
            "\t Validation loss 0.00567, Validation accuracy 79.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 2\n",
            "\t Training loss 0.00378, Training accuracy 85.17\n",
            "\t Validation loss 0.00485, Validation accuracy 82.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 3\n",
            "\t Training loss 0.00308, Training accuracy 87.94\n",
            "\t Validation loss 0.00470, Validation accuracy 84.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 4\n",
            "\t Training loss 0.00286, Training accuracy 88.67\n",
            "\t Validation loss 0.00492, Validation accuracy 84.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 5\n",
            "\t Training loss 0.00268, Training accuracy 89.59\n",
            "\t Validation loss 0.00462, Validation accuracy 85.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 6\n",
            "\t Training loss 0.00240, Training accuracy 90.66\n",
            "\t Validation loss 0.00458, Validation accuracy 84.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 7\n",
            "\t Training loss 0.00243, Training accuracy 90.90\n",
            "\t Validation loss 0.00401, Validation accuracy 88.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 8\n",
            "\t Training loss 0.00245, Training accuracy 90.49\n",
            "\t Validation loss 0.00394, Validation accuracy 87.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 9\n",
            "\t Training loss 0.00224, Training accuracy 91.14\n",
            "\t Validation loss 0.00380, Validation accuracy 89.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 10\n",
            "\t Training loss 0.00226, Training accuracy 91.79\n",
            "\t Validation loss 0.00397, Validation accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "Epoch: 11\n",
            "\t Training loss 0.00206, Training accuracy 92.47\n",
            "\t Validation loss 0.00355, Validation accuracy 90.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 12\n",
            "\t Training loss 0.00210, Training accuracy 92.09\n",
            "\t Validation loss 0.00401, Validation accuracy 87.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 13\n",
            "\t Training loss 0.00177, Training accuracy 93.50\n",
            "\t Validation loss 0.00372, Validation accuracy 90.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 14\n",
            "\t Training loss 0.00165, Training accuracy 93.49\n",
            "\t Validation loss 0.00365, Validation accuracy 90.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 15\n",
            "\t Training loss 0.00198, Training accuracy 92.57\n",
            "\t Validation loss 0.00468, Validation accuracy 88.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 16\n",
            "\t Training loss 0.00205, Training accuracy 92.72\n",
            "\t Validation loss 0.00418, Validation accuracy 89.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 17\n",
            "\t Training loss 0.00210, Training accuracy 92.69\n",
            "\t Validation loss 0.00434, Validation accuracy 88.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 18\n",
            "\t Training loss 0.00217, Training accuracy 92.14\n",
            "\t Validation loss 0.00471, Validation accuracy 89.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 19\n",
            "\t Training loss 0.00191, Training accuracy 93.62\n",
            "\t Validation loss 0.00446, Validation accuracy 91.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 20\n",
            "\t Training loss 0.00175, Training accuracy 93.70\n",
            "\t Validation loss 0.00397, Validation accuracy 91.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 21\n",
            "\t Training loss 0.00214, Training accuracy 92.46\n",
            "\t Validation loss 0.00510, Validation accuracy 89.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 22\n",
            "\t Training loss 0.00219, Training accuracy 92.72\n",
            "\t Validation loss 0.00461, Validation accuracy 89.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 23\n",
            "\t Training loss 0.00195, Training accuracy 92.90\n",
            "\t Validation loss 0.00508, Validation accuracy 88.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 24\n",
            "\t Training loss 0.00191, Training accuracy 93.72\n",
            "\t Validation loss 0.00519, Validation accuracy 89.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 25\n",
            "\t Training loss 0.00168, Training accuracy 94.03\n",
            "\t Validation loss 0.00485, Validation accuracy 88.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 26\n",
            "\t Training loss 0.00147, Training accuracy 94.50\n",
            "\t Validation loss 0.00456, Validation accuracy 89.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 27\n",
            "\t Training loss 0.00170, Training accuracy 94.12\n",
            "\t Validation loss 0.00438, Validation accuracy 91.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 28\n",
            "\t Training loss 0.00175, Training accuracy 93.72\n",
            "\t Validation loss 0.00465, Validation accuracy 90.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 29\n",
            "\t Training loss 0.00159, Training accuracy 94.13\n",
            "\t Validation loss 0.00534, Validation accuracy 89.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 30\n",
            "\t Training loss 0.00188, Training accuracy 93.46\n",
            "\t Validation loss 0.00504, Validation accuracy 90.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 31\n",
            "\t Training loss 0.00198, Training accuracy 93.59\n",
            "\t Validation loss 0.00489, Validation accuracy 90.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 32\n",
            "\t Training loss 0.00148, Training accuracy 94.24\n",
            "\t Validation loss 0.00474, Validation accuracy 89.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 33\n",
            "\t Training loss 0.00151, Training accuracy 94.36\n",
            "\t Validation loss 0.00503, Validation accuracy 89.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 34\n",
            "\t Training loss 0.00136, Training accuracy 95.30\n",
            "\t Validation loss 0.00471, Validation accuracy 90.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 35\n",
            "\t Training loss 0.00121, Training accuracy 95.67\n",
            "\t Validation loss 0.00431, Validation accuracy 91.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 36\n",
            "\t Training loss 0.00135, Training accuracy 95.09\n",
            "\t Validation loss 0.00476, Validation accuracy 90.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 37\n",
            "\t Training loss 0.00131, Training accuracy 95.41\n",
            "\t Validation loss 0.00453, Validation accuracy 91.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 38\n",
            "\t Training loss 0.00141, Training accuracy 94.76\n",
            "\t Validation loss 0.00483, Validation accuracy 91.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 39\n",
            "\t Training loss 0.00163, Training accuracy 94.54\n",
            "\t Validation loss 0.00543, Validation accuracy 90.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 40\n",
            "\t Training loss 0.00220, Training accuracy 93.33\n",
            "\t Validation loss 0.00631, Validation accuracy 89.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 41\n",
            "\t Training loss 0.00230, Training accuracy 92.84\n",
            "\t Validation loss 0.00533, Validation accuracy 90.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 42\n",
            "\t Training loss 0.00159, Training accuracy 94.96\n",
            "\t Validation loss 0.00541, Validation accuracy 90.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 43\n",
            "\t Training loss 0.00174, Training accuracy 94.52\n",
            "\t Validation loss 0.00514, Validation accuracy 90.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 44\n",
            "\t Training loss 0.00137, Training accuracy 95.10\n",
            "\t Validation loss 0.00448, Validation accuracy 92.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 45\n",
            "\t Training loss 0.00173, Training accuracy 94.54\n",
            "\t Validation loss 0.00452, Validation accuracy 91.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 46\n",
            "\t Training loss 0.00214, Training accuracy 93.26\n",
            "\t Validation loss 0.00634, Validation accuracy 88.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 47\n",
            "\t Training loss 0.00190, Training accuracy 94.32\n",
            "\t Validation loss 0.00536, Validation accuracy 90.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 48\n",
            "\t Training loss 0.00140, Training accuracy 95.24\n",
            "\t Validation loss 0.00426, Validation accuracy 91.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 49\n",
            "\t Training loss 0.00123, Training accuracy 95.63\n",
            "\t Validation loss 0.00443, Validation accuracy 91.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 50\n",
            "\t Training loss 0.00144, Training accuracy 94.89\n",
            "\t Validation loss 0.00530, Validation accuracy 89.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 51\n",
            "\t Training loss 0.00138, Training accuracy 94.97\n",
            "\t Validation loss 0.00538, Validation accuracy 90.07\n",
            "-----------------------------------------------------\n",
            "Epoch: 52\n",
            "\t Training loss 0.00146, Training accuracy 94.99\n",
            "\t Validation loss 0.00506, Validation accuracy 90.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 53\n",
            "\t Training loss 0.00147, Training accuracy 95.14\n",
            "\t Validation loss 0.00557, Validation accuracy 89.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 54\n",
            "\t Training loss 0.00161, Training accuracy 94.72\n",
            "\t Validation loss 0.00550, Validation accuracy 90.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 55\n",
            "\t Training loss 0.00150, Training accuracy 95.13\n",
            "\t Validation loss 0.00535, Validation accuracy 91.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 56\n",
            "\t Training loss 0.00177, Training accuracy 94.99\n",
            "\t Validation loss 0.00525, Validation accuracy 90.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 57\n",
            "\t Training loss 0.00171, Training accuracy 94.66\n",
            "\t Validation loss 0.00571, Validation accuracy 90.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 58\n",
            "\t Training loss 0.00188, Training accuracy 94.40\n",
            "\t Validation loss 0.00556, Validation accuracy 89.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 59\n",
            "\t Training loss 0.00131, Training accuracy 95.60\n",
            "\t Validation loss 0.00539, Validation accuracy 90.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 60\n",
            "\t Training loss 0.00143, Training accuracy 95.53\n",
            "\t Validation loss 0.00568, Validation accuracy 89.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 61\n",
            "\t Training loss 0.00128, Training accuracy 95.51\n",
            "\t Validation loss 0.00553, Validation accuracy 90.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 62\n",
            "\t Training loss 0.00126, Training accuracy 95.90\n",
            "\t Validation loss 0.00567, Validation accuracy 90.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 63\n",
            "\t Training loss 0.00145, Training accuracy 95.57\n",
            "\t Validation loss 0.00633, Validation accuracy 89.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 64\n",
            "\t Training loss 0.00141, Training accuracy 95.71\n",
            "\t Validation loss 0.00546, Validation accuracy 91.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 65\n",
            "\t Training loss 0.00150, Training accuracy 95.49\n",
            "\t Validation loss 0.00610, Validation accuracy 90.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 66\n",
            "\t Training loss 0.00118, Training accuracy 96.17\n",
            "\t Validation loss 0.00581, Validation accuracy 90.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 67\n",
            "\t Training loss 0.00158, Training accuracy 95.10\n",
            "\t Validation loss 0.00588, Validation accuracy 90.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 68\n",
            "\t Training loss 0.00152, Training accuracy 95.23\n",
            "\t Validation loss 0.00607, Validation accuracy 89.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 69\n",
            "\t Training loss 0.00155, Training accuracy 95.13\n",
            "\t Validation loss 0.00453, Validation accuracy 91.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 70\n",
            "\t Training loss 0.00153, Training accuracy 94.86\n",
            "\t Validation loss 0.00557, Validation accuracy 90.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 71\n",
            "\t Training loss 0.00153, Training accuracy 95.07\n",
            "\t Validation loss 0.00503, Validation accuracy 90.33\n",
            "-----------------------------------------------------\n",
            "Epoch: 72\n",
            "\t Training loss 0.00140, Training accuracy 95.30\n",
            "\t Validation loss 0.00460, Validation accuracy 91.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 73\n",
            "\t Training loss 0.00144, Training accuracy 95.67\n",
            "\t Validation loss 0.00553, Validation accuracy 90.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 74\n",
            "\t Training loss 0.00135, Training accuracy 95.51\n",
            "\t Validation loss 0.00604, Validation accuracy 90.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 75\n",
            "\t Training loss 0.00124, Training accuracy 95.91\n",
            "\t Validation loss 0.00516, Validation accuracy 90.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 76\n",
            "\t Training loss 0.00146, Training accuracy 95.40\n",
            "\t Validation loss 0.00530, Validation accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 77\n",
            "\t Training loss 0.00143, Training accuracy 94.93\n",
            "\t Validation loss 0.00489, Validation accuracy 91.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 78\n",
            "\t Training loss 0.00135, Training accuracy 95.21\n",
            "\t Validation loss 0.00522, Validation accuracy 90.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 79\n",
            "\t Training loss 0.00170, Training accuracy 94.76\n",
            "\t Validation loss 0.00476, Validation accuracy 90.47\n",
            "-----------------------------------------------------\n",
            "Epoch: 80\n",
            "\t Training loss 0.00137, Training accuracy 95.34\n",
            "\t Validation loss 0.00450, Validation accuracy 91.27\n",
            "-----------------------------------------------------\n",
            "Epoch: 81\n",
            "\t Training loss 0.00154, Training accuracy 94.70\n",
            "\t Validation loss 0.00527, Validation accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 82\n",
            "\t Training loss 0.00135, Training accuracy 95.49\n",
            "\t Validation loss 0.00496, Validation accuracy 90.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 83\n",
            "\t Training loss 0.00169, Training accuracy 94.80\n",
            "\t Validation loss 0.00536, Validation accuracy 90.13\n",
            "-----------------------------------------------------\n",
            "Epoch: 84\n",
            "\t Training loss 0.00143, Training accuracy 95.20\n",
            "\t Validation loss 0.00488, Validation accuracy 91.20\n",
            "-----------------------------------------------------\n",
            "Epoch: 85\n",
            "\t Training loss 0.00136, Training accuracy 95.30\n",
            "\t Validation loss 0.00506, Validation accuracy 90.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 86\n",
            "\t Training loss 0.00136, Training accuracy 95.17\n",
            "\t Validation loss 0.00461, Validation accuracy 91.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 87\n",
            "\t Training loss 0.00145, Training accuracy 95.31\n",
            "\t Validation loss 0.00471, Validation accuracy 90.73\n",
            "-----------------------------------------------------\n",
            "Epoch: 88\n",
            "\t Training loss 0.00128, Training accuracy 95.79\n",
            "\t Validation loss 0.00503, Validation accuracy 91.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 89\n",
            "\t Training loss 0.00140, Training accuracy 95.33\n",
            "\t Validation loss 0.00487, Validation accuracy 90.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 90\n",
            "\t Training loss 0.00190, Training accuracy 94.00\n",
            "\t Validation loss 0.00630, Validation accuracy 87.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 91\n",
            "\t Training loss 0.00205, Training accuracy 94.17\n",
            "\t Validation loss 0.00604, Validation accuracy 89.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 92\n",
            "\t Training loss 0.00153, Training accuracy 95.30\n",
            "\t Validation loss 0.00490, Validation accuracy 91.67\n",
            "-----------------------------------------------------\n",
            "Epoch: 93\n",
            "\t Training loss 0.00152, Training accuracy 95.23\n",
            "\t Validation loss 0.00564, Validation accuracy 91.53\n",
            "-----------------------------------------------------\n",
            "Epoch: 94\n",
            "\t Training loss 0.00144, Training accuracy 95.43\n",
            "\t Validation loss 0.00479, Validation accuracy 91.80\n",
            "-----------------------------------------------------\n",
            "Epoch: 95\n",
            "\t Training loss 0.00166, Training accuracy 95.44\n",
            "\t Validation loss 0.00596, Validation accuracy 89.87\n",
            "-----------------------------------------------------\n",
            "Epoch: 96\n",
            "\t Training loss 0.00178, Training accuracy 94.69\n",
            "\t Validation loss 0.00526, Validation accuracy 91.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 97\n",
            "\t Training loss 0.00128, Training accuracy 95.87\n",
            "\t Validation loss 0.00497, Validation accuracy 92.40\n",
            "-----------------------------------------------------\n",
            "Epoch: 98\n",
            "\t Training loss 0.00141, Training accuracy 95.37\n",
            "\t Validation loss 0.00515, Validation accuracy 90.60\n",
            "-----------------------------------------------------\n",
            "Epoch: 99\n",
            "\t Training loss 0.00132, Training accuracy 95.59\n",
            "\t Validation loss 0.00496, Validation accuracy 91.00\n",
            "-----------------------------------------------------\n",
            "Epoch: 100\n",
            "\t Training loss 0.00135, Training accuracy 95.89\n",
            "\t Validation loss 0.00582, Validation accuracy 89.93\n",
            "-----------------------------------------------------\n",
            "After training:\n",
            "\t Training loss 0.00142, Training accuracy 95.03\n",
            "\t Validation loss 0.00582, Validation accuracy 89.93\n",
            "\t Test loss 0.00170, Test accuracy 92.60\n",
            "-----------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}